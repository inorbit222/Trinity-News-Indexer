Installing the Dependencies
To install all the dependencies, simply run the following command in your terminal:

pip install -r requirements.txt

Explanation of Key Packages:
psycopg2: Used for connecting and interacting with your PostgreSQL database.
gensim: Provides tools for topic modeling, especially useful for LDA.
sentence-transformers: Used to generate embeddings for articles.
faiss-cpu: FAISS library for efficient similarity searches with embeddings.
vaderSentiment: Sentiment analysis library, pre-trained for social media and news text.
geopy: Geocoding library used to extract location data from named entities.
nltk: Provides tokenization, stopwords, and other NLP-related tools.
transformers: Used for Named Entity Recognition (NER) and other transformer-based models.
torch: Required by Hugging Face transformers for model inference.
numpy: Used by many libraries for array manipulations and numerical computations.


README for First Three Preprocessing Scripts:
This section of the pipeline focuses on preparing raw newspaper text files and dividing them into more structured and manageable articles for further processing. Each step has its own script, which transforms the input text data. Below is a detailed explanation of each step:

1. Raw Text to Segmented Files
Script: 1_article_divider.py
Description: This script takes raw newspaper text files, identifies the structure of articles, and segments them based on the detected titles and bodies of the articles.
Key Tasks:
Reads raw text files.
Uses regular expressions to detect article titles and bodies.
Outputs each article to a structured file with clear title-body separation.
Adjustable Variables:
input_directory: Path to the directory where raw text files are located.
output_directory: Path to the directory where segmented files will be stored.
Usage:
bash

python 1_article_divider.py
Sample Output Format:

====================================
Title: Example Article Title
Body:
This is the body of the example article...
====================================

2. Segmented Files (Step 1) to Step 2
Script: 2_article_divider.py
Description: This script further processes the segmented files generated by step 1. It ensures that articles are structured properly with titles and bodies separated by a delimiter. It works by refining the segmentation done in the previous step.
Key Tasks:
Detects article boundaries.
Ensures each article has a title, even if it's extracted from the first few words of the body when the title is missing.
Saves structured articles into a new directory.
Adjustable Variables:
input_directory: Path to the directory containing files processed by the first script.
output_directory: Path to the directory where further processed files will be saved.
Usage:
bash

python 2_article_divider.py

Sample Output:
vbnet

====================================
Title: Example Article Title (or first five words of body)
Body:
This is the refined body text of the article...
====================================

3. Step 3 to Segmented Files (Final Step)
Script: 3_article_divider.py

Description: This script completes the segmentation process by ensuring that any remaining loose text fragments are organized into articles. It generates a final, segmented format that will be used in subsequent processing steps.
Key Tasks:
Refines remaining text into structured articles.
Detects title-body pairs and fills in missing titles where necessary.
Prepares the segmented articles for database entry.
Adjustable Variables:
input_directory: Directory containing files from step 2.
output_directory: Directory where the final segmented files are saved.
Usage:
bash

python 3_article_divider.py

Key Variables for Fine-Tuning:
Regular Expressions for Title and Body Detection: You can modify the regular expressions used to detect article titles and bodies depending on the structure of the raw newspaper files.
Default Title Generation: If an article does not have a clear title, the scripts will extract the first five words from the body to use as a placeholder title. This behavior can be customized by adjusting the logic within the script.

***********************************************************
Preprocessing Complete, Next 7 steps will populate database.
***********************************************************

Newspaper Data Processing Pipeline
This pipeline processes segmented historical newspaper articles by performing a series of tasks, including Named Entity Recognition (NER), Latent Dirichlet Allocation (LDA) topic modeling, sentiment analysis, FAISS indexing for similarity search, and geocoding. The pipeline stores results in a PostgreSQL database.

Pipeline Overview

1. Segmented Files to Database
Script: 4_segmented_to_db.py
Description: Processes raw segmented newspaper files and stores them in the Articles and Newspapers tables.

Adjustable Variables:
segmented_dir: Path to the directory containing segmented text files. Update this variable to point to your data directory.

Date Parsing: The script extracts publication dates from file names. Ensure the filename format matches the date pattern used in the script.

2. Sentence Transformers (Embeddings) to Database
Script: 5_sentence_transformer.py
Description: Generates embeddings for articles using the Sentence Transformers model and stores them in the embedding_vector column of the Articles table.

Adjustable Variables:
Sentence Transformer Model: You can change the model used by the SentenceTransformer. For example, replace 'sentence-transformers/gtr-t5-large' with any other transformer model.

Embedding Dimensionality: If you switch models, make sure the embedding dimensionality matches the FAISS index setup.

3. Named Entity Recognition (NER) to Database
Script: 6_NER_to_database.py
Description: Extracts named entities from articles using a pre-trained Hugging Face NER model and stores the results in the Entities table.

Adjustable Variables:
NER Model: The script uses the 'dbmdz/bert-large-cased-finetuned-conll03-english' model by default. You can replace this with any other NER model compatible with Hugging Face's transformers.

4. Latent Dirichlet Allocation (LDA) to Database
Script: 7_LDA_to_DB.py
Description: Performs topic modeling using LDA on the article content and stores the results in the Topics and Article_Topics tables.

Adjustable Variables:
num_topics: You can adjust the number of topics by changing this parameter in the train_lda_model() function. Increasing this value will produce more granular topics.

LDA Model Passes: The number of training passes (set to 15) can be increased for more accurate topic modeling.


5. Sentiment Analysis to Database
Script: 8_sentiment_analysis_to_DB.py
Description: Analyzes sentiment for each entity in the Entities table using the VADER sentiment analysis tool and stores the results in the entity_sentiments table.

Adjustable Variables:
Sentiment Analyzer: This script uses VADER by default. You can replace VADER with a different sentiment analysis tool (e.g., TextBlob) if you need different sentiment metrics.

Granularity of Sentiment: Adjust the level of sentiment granularity by analyzing entities or larger text blocks such as entire articles.


6. FAISS to Database (Similarity Search)
Script: FAISS_to_database.py
Description: Creates a FAISS index for article embeddings and stores the index to enable similarity search between articles.

Adjustable Variables:
Embedding Dimension: Ensure the FAISS index dimensionality matches the embeddings (default is 768 for BERT-like models).

Distance Metric: The script uses L2 distance by default (IndexFlatL2). You can switch to cosine similarity or another distance metric if needed.


7. Geocoding to Database
Script: GEO_to_database.py
Description: Geocodes location entities (entities tagged as GPE by the NER model) using the Nominatim geocoding service and stores latitude and longitude in the Geocoded_Locations table.

Adjustable Variables:
Geocoding Service: The script uses Nominatim (OpenStreetMap) for geocoding. You can replace it with another geocoding service (e.g., Google Maps API or Mapbox).

Geocoding Rate Limiting: Nominatim has rate limits. Ensure you respect those or switch to a paid service for higher limits.


Database Schema
The following tables are created to store the results of each processing step:

Newspapers: Stores metadata about newspapers.
Articles: Stores article content, title, newspaper link, and embeddings.
Entities: Stores named entities extracted from articles, including entity type and position.
Topics: Stores LDA topics with descriptions.
Article_Topics: Stores topic weights associated with articles.
Geocoded_Locations: Stores latitude and longitude data for geocoded entities.
entity_sentiments: Stores sentiment scores (positive, negative, neutral, compound) for entities.
faiss_index: Stores article embeddings for FAISS indexing.
How to Run the Pipeline

1. Make sure dependencies are installed.
Run the

2. Set Up the Database
Before running any scripts, make sure the PostgreSQL database is set up. Run the Database Schema Creation script to create the required tables.

bash
Copy code
python Database_Creator.py
3. Run Each Script in Order
To process the data, run each script in order:

Segmented Files to Database:

bash
Copy code
python 4_segmented_to_db.py
Sentence Transformers (Embeddings):

bash
Copy code
python 5_sentence_transformer.py
Named Entity Recognition (NER):

bash
Copy code
python 6_NER_to_database.py
Latent Dirichlet Allocation (LDA):

bash
Copy code
python 7_LDA_to_DB.py
Sentiment Analysis:

bash
Copy code
python 8_sentiment_analysis_to_DB.py
FAISS Indexing:

bash
Copy code
python FAISS_to_database.py
Geocoding:

bash
Copy code
python GEO_to_database.py
Fine-Tuning the Pipeline
Each script contains variables that you can modify to adjust the behavior and output. The key variables include:

Model Selection: Replace pre-trained models in the NER, embeddings, or sentiment analysis scripts with different models for better results.

Topic Granularity: Adjust the number of topics in the LDA script to create more or fewer categories.
Embedding Dimensionality: Ensure the dimensionality of the embeddings matches the FAISS index structure.

Troubleshooting
Database Connection Issues: Ensure the PostgreSQL database is running and that the credentials in each script match your local setup.

Model Errors: Ensure the models you use in the Sentence Transformer and NER scripts are compatible with Hugging Face and have the correct input/output formats.

Rate Limiting: If using Nominatim for geocoding, be aware of rate limits imposed by the service. Switch to a paid geocoding service if needed.# Trinity-News-Indexer
